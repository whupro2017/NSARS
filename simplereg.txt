#Linear regression
fit<-lm(weight~height,data=women)
#formula(Y~X1+X2+XK)
#myfit<-lm(formula, data)
formula = height ~ weight
myfit<-lm(formula, data=women)
women$weight

#Binomial regresion
fit2<-lm(weight~height + I(height^2), data=women)
fit21<-lm(weight~height + height^2, data=women)
#f2<-lm(weight~height, data=women)

#Multinomial regression
fit3<-lm(weight~height + I(height^2) + I(height^3), data=women)
fit31<-lm(weight~height + I(height^2) + I(height^2), data=women)

install.packages("raster")
dev.new(width=6,height=6)
fit<-lm(Beijing~Time + I(Time^2) + I(Time ^ 3) + I(Time^4) + I(Time ^ 5), data=city)
plot(city[,c("Beijing")] ~ city[,c("Time")], data = city, col = 'red', type = 'o', xlim = c(0, 80), ylim = c(1,300))
par(new=TRUE)
plot(fitted(fit)~city[,c("Time")], col='blue', type='o',xlim=c(0,80),ylim=c(1,300))
result<-data.frame(Time=c(34:80),Result=predict(fit, data.frame(Time=c(34:80))))
par(new=TRUE)
plot(Result~Time, data=result,col = 'green', type = 'o', xlim = c(0, 80), ylim = c(1,300))

scatterplot(weight~height, data=women, spread=FALSE, smoother.args=list(Ity=2), pch=19, main="Women Age 30-39", xlab="Height(inches)",ylab="Weight(lbs.)")

#Multivariant linear regression
fitm<-lm(Murder~Population+Illiteracy+Income+Frost, data=states)
summary(fit)

states<-as.data.frame(state.x77[,c("Murder","Population","Illiteracy","Income","Frost")])
cor(states)

library(effects)
fitm<-lm(Murder~Population+Illiteracy+Income+Frost, data=states)
scatterplot(Murder~Population+Illiteracy+Income+Frost, data=states, spread=FALSE, smoother.args=list(Ity=2), pch=19, main="Women Age 30-39", xlab="Height(inches)",ylab="Weight(lbs.)")

#Interactive multivariant linear regression
fiti<- lm(mpg ~ hp+ wt +hp:wt, data=mtcars)
library(carData)
plot(effect("hp:wt", fiti,, list(wt=c(2.2,3.2,4.2))), multiline=TRUE)

#Logical regression
library(pROC)
library(DMwR)
	#model.data summary
model.df<-city
dim(model.df)
head(model.df)
str(model.df)
summary(model.df)
	#remove Nas
z <- model.df[,sapply(model.df, is.numeric)]
z[is.na(z)] = 0
summary(z)
	#Maximum by quantile 99% and Minimum by quantile 1%
qs <- sapply(z, function(z) quantile(z,c(0.01,0.99)))
system.time(for (i in 1:ncol(z)){
  for( j in 1:nrow(z)){
    if(z[j,i] < qs[1,i]) z[j,i] = qs[1,i]
    if(z[j,i] > qs[2,i]) z[j,i] = qs[2,i]
  }
})
	#Rebase data
model_ad.df <- data.frame(cust_id=model.df$Time,defect=model.df$Beijing,z)
boxplot(model_ad.df$Beijing)
set.seed(123)
	#Extracting training/testing datasets
s <- sample(nrow(model_ad.df),floor(nrow(model_ad.df)*0.7),replace = F)
train_df <- model_ad.df[s,]
test_df <- model_ad.df[-s,]
	#Remove cust_id
n <- names(train_df[-c(1,35)])
	#Formular of logical regression
#f <- as.formula(paste('defect ~',paste(n[!n %in% 'defect'],collapse = ' + ')))
f<-as.formula(Beijing~Time+I(Time^2) + I(Time^3))
#model_full <- glm(f,data=train_df[-c(1,35)],family = binomial)#poisson gaussian
#model_full <- glm(f,data=train_df[c(3,5)],family = gaussian)
model_full <- glm(f,data=model.df[c(1,3)],family = gaussian)
#	#Obtain minimum AIC
step <- step(model_full,direction='both')
# summary(step)
# 	#Predict
# pred <- predict(step,test_df,type='response')
# head(pred)
# fitted.r <- ifelse(pred>0.5,1,0)
#	#accuracy
# accuracy <- table(fitted.r,test_df$defect)
# 	#roc
# #roc <- roc(test_df$defect,pred)
# #roc
# #plot(roc)
	#Plot
dev.new(width=6,height=6)
len=80
#fit<-lm(Beijing~Time + I(Time^2) + I(Time ^ 3) + I(Time^4) + I(Time ^ 5), data=city)
plot(city[,c("Beijing")] ~ city[,c("Time")], data = city, col = 'red', type = 'o', xlim = c(0, len), ylim = c(1, 6000))
par(new=TRUE)
plot(fitted(model_full)~city[,c("Time")], col='blue', type='o',xlim=c(0, len),ylim=c(1, 6000))
result<-data.frame(Time=c(35:len),Result=predict(model_full, data.frame(Time=c(35: len))))
par(new=TRUE)
plot(Result~Time, data=result,col = 'green', type = 'o', xlim = c(0, len), ylim = c(1, 6000))

#Ridge regression
lm.sol <- lm(Beijing ~ Time, data = city)
